{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (vision.py, line 56)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3505\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 25\u001b[0;36m\n\u001b[0;31m    from src.models.vision import DinoEncoder\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/furniture-diffusion/src/models/vision.py:56\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import zarr\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from furniture_bench.robot.robot_state import filter_and_concat_robot_state\n",
    "from furniture_bench.perception.image_utils import resize_crop\n",
    "\n",
    "from vip import load_vip\n",
    "\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "from ipdb import set_trace as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"/home/larsankile/furniture-diffusion/data/\")\n",
    "randomness = \"low\"\n",
    "furniture = \"one_leg\"\n",
    "extension = \".tar.gz\"\n",
    "obs_type = \"image\"\n",
    "env_type = \"real\"\n",
    "filename = furniture + extension\n",
    "\n",
    "\n",
    "input_file = root / \"raw\" / env_type / obs_type / randomness / filename\n",
    "output_file = (\n",
    "    root / \"processed\" / env_type / obs_type / randomness / furniture / \"data.zarr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 200\n",
    "\n",
    "raw_data, n_samples = [], 0\n",
    "with tarfile.open(input_file, \"r:gz\") as tar:\n",
    "    for member in tqdm(tar, desc=f\"Extracting {furniture}\", total=max_samples):\n",
    "        if (\n",
    "            member.isfile() and \".pkl\" in member.name\n",
    "        ):  # Replace 'your_condition' with actual condition\n",
    "            with tar.extractfile(member) as f:\n",
    "                if f is not None:\n",
    "                    content = f.read()\n",
    "                    data = pickle.loads(content)\n",
    "                    raw_data.append(data)\n",
    "                    n_samples += 1\n",
    "\n",
    "                    if n_samples >= max_samples:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(\"/home/larsankile/furniture-diffusion/data/raw/sim/full/one_leg/low\")\n",
    "output_file = Path(\n",
    "    \"/home/larsankile/furniture-diffusion/data/processed/sim/image/low/one_leg/data.zarr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:03<00:00, 30.14it/s]\n"
     ]
    }
   ],
   "source": [
    "files = list(input_dir.rglob(\"**/*.pkl\"))\n",
    "\n",
    "raw_data = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        raw_data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vip = load_vip(device_id=1).module\n",
    "\n",
    "vip.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(img_batch, encoder):\n",
    "    with torch.no_grad():\n",
    "        img_tensor = torch.tensor(\n",
    "            img_batch, dtype=torch.float32, device=encoder.device\n",
    "        ).permute(0, 3, 1, 2)\n",
    "        features = encoder(img_tensor).cpu().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "observations = []\n",
    "actions = []\n",
    "episode_ends = []\n",
    "\n",
    "end_index = 0\n",
    "\n",
    "for data in tqdm(raw_data):\n",
    "    img1_batch = []\n",
    "    img2_batch = []\n",
    "    for obs, action in zip(data[\"observations\"], data[\"actions\"]):\n",
    "        robot_state = filter_and_concat_robot_state(obs[\"robot_state\"])\n",
    "\n",
    "        img1_batch.append(obs[\"color_image1\"])\n",
    "        img2_batch.append(obs[\"color_image2\"])\n",
    "\n",
    "        actions.append(action)\n",
    "\n",
    "        if len(img1_batch) == batch_size:\n",
    "            img1_features = get_features(np.stack(img1_batch, axis=0))\n",
    "            img2_features = get_features(np.stack(img2_batch, axis=0))\n",
    "\n",
    "            for f1, f2 in zip(img1_features, img2_features):\n",
    "                observation = np.concatenate((robot_state, f1, f2))\n",
    "                observations.append(observation)\n",
    "\n",
    "            img1_batch = []\n",
    "            img2_batch = []\n",
    "\n",
    "        end_index += 1\n",
    "\n",
    "    # Handle any remaining images within each trajectory\n",
    "    if img1_batch:\n",
    "        img1_features = get_features(np.stack(img1_batch, axis=0))\n",
    "        img2_features = get_features(np.stack(img2_batch, axis=0))\n",
    "\n",
    "        for f1, f2 in zip(img1_features, img2_features):\n",
    "            observation = np.concatenate((robot_state, f1, f2))\n",
    "            observations.append(observation)\n",
    "\n",
    "    episode_ends.append(end_index)\n",
    "\n",
    "observations = np.array(observations)\n",
    "actions = np.array(actions)\n",
    "episode_ends = np.array(episode_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.save(\n",
    "    output_file,\n",
    "    observations=observations,\n",
    "    actions=actions,\n",
    "    episode_ends=episode_ends,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert existing zarr image dataset to feature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = Path(\n",
    "    \"/home/larsankile/furniture-diffusion/data/processed/sim/image/low/one_leg/data.zarr\"\n",
    ")\n",
    "output_file = Path(\n",
    "    \"/home/larsankile/furniture-diffusion/data/processed/sim/feature/dino/low/one_leg/data.zarr\"\n",
    ")\n",
    "\n",
    "data = zarr.open(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DinoEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/larsankile/furniture-diffusion/notebooks/04_la_process_demos.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Banthony-gpu/home/larsankile/furniture-diffusion/notebooks/04_la_process_demos.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m encoder \u001b[39m=\u001b[39m DinoEncoder()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DinoEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = DinoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45876, 45876, 45876)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"image1\"]), len(data[\"image2\"]), len(data[\"agent_pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [18:55<00:00, 12.61s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "feat1_batches = []\n",
    "feat2_batches = []\n",
    "\n",
    "for i in trange(0, len(data[\"image1\"]), batch_size):\n",
    "    img1_features = get_features(data[\"image1\"][i : i + batch_size], encoder)\n",
    "    img2_features = get_features(data[\"image2\"][i : i + batch_size], encoder)\n",
    "\n",
    "    feat1_batches.append(img1_features)\n",
    "    feat2_batches.append(img2_features)\n",
    "\n",
    "feat1 = np.concatenate(feat1_batches, axis=0)\n",
    "feat2 = np.concatenate(feat2_batches, axis=0)\n",
    "\n",
    "# Concatenate features with robot state\n",
    "agent_pos = data[\"agent_pos\"][:]\n",
    "observations = np.concatenate((agent_pos, feat1, feat2), axis=1)\n",
    "\n",
    "zarr.save(\n",
    "    output_file,\n",
    "    observations=observations,\n",
    "    actions=data[\"actions\"][:],\n",
    "    episode_ends=data[\"episode_ends\"][:],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:00<00:00, 533.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((45876, 14), (45876, 224, 224, 3), (45876, 224, 224, 3), (45876, 8), (94,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_pos = []\n",
    "image1 = []\n",
    "image2 = []\n",
    "actions = []\n",
    "episode_ends = []\n",
    "\n",
    "\n",
    "for data in tqdm(raw_data):\n",
    "    img1_batch = []\n",
    "    img2_batch = []\n",
    "    for agent_pos, image1, image2 in zip(data[\"observations\"], data[\"actions\"]):\n",
    "        robot_state = filter_and_concat_robot_state(obs[\"robot_state\"])\n",
    "        agent_pos.append(robot_state)\n",
    "\n",
    "        img1 = obs[\"color_image1\"]\n",
    "        img2 = obs[\"color_image2\"]\n",
    "\n",
    "        if img1.shape != (224, 224, 3):\n",
    "            img1 = resize_crop(img1)\n",
    "            img2 = resize_crop(img2)\n",
    "\n",
    "        image1.append(img1)\n",
    "        image2.append(img2)\n",
    "\n",
    "        actions.append(action)\n",
    "\n",
    "        end_index += 1\n",
    "\n",
    "    episode_ends.append(end_index)\n",
    "\n",
    "agent_pos = np.array(agent_pos)\n",
    "image1 = np.array(image1)\n",
    "image2 = np.array(image2)\n",
    "actions = np.array(actions)\n",
    "episode_ends = np.array(episode_ends)\n",
    "\n",
    "agent_pos.shape, image1.shape, image2.shape, actions.shape, episode_ends.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/larsankile/furniture-diffusion/data/processed/sim/image/low/one_leg/data.zarr')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.save(\n",
    "    output_file,\n",
    "    agent_pos=agent_pos,\n",
    "    image1=image1,\n",
    "    image2=image2,\n",
    "    actions=actions,\n",
    "    episode_ends=episode_ends,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
