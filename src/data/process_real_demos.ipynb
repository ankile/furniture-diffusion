{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import zarr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from furniture_bench.robot.robot_state import filter_and_concat_robot_state\n",
    "\n",
    "from vip import load_vip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"/home/larsankile/furniture-diffusion/data/\")\n",
    "randomness = \"low\"\n",
    "furniture = \"lamp\"\n",
    "extension = \".tar.gz\"\n",
    "filename = furniture + extension\n",
    "\n",
    "\n",
    "input_file = root / \"raw\" / \"real\" / \"image\" / randomness / filename\n",
    "output_file = (\n",
    "    root / \"processed\" / \"real\" / \"feature\" / randomness / furniture / \"data.zarr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting lamp:   0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting lamp: 151it [02:45,  1.10s/it]                         \n"
     ]
    }
   ],
   "source": [
    "raw_data = []\n",
    "\n",
    "with tarfile.open(input_file, \"r:gz\") as tar:\n",
    "    for member in tqdm(tar, desc=\"Extracting lamp\", total=150):\n",
    "        if (\n",
    "            member.isfile() and \".pkl\" in member.name\n",
    "        ):  # Replace 'your_condition' with actual condition\n",
    "            with tar.extractfile(member) as f:\n",
    "                if f is not None:\n",
    "                    content = f.read()\n",
    "                    data = pickle.loads(content)\n",
    "                    raw_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vip = load_vip(device_id=1).module\n",
    "\n",
    "vip.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(img_batch):\n",
    "    with torch.no_grad():\n",
    "        img_tensor = torch.tensor(\n",
    "            img_batch, dtype=torch.float32, device=\"cuda:1\"\n",
    "        ).permute(0, 3, 1, 2)\n",
    "        features = vip(img_tensor).cpu().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [04:18<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "observations = []\n",
    "actions = []\n",
    "episode_ends = []\n",
    "\n",
    "end_index = 0\n",
    "\n",
    "for data in tqdm(raw_data):\n",
    "    img1_batch = []\n",
    "    img2_batch = []\n",
    "    for obs, action in zip(data[\"observations\"], data[\"actions\"]):\n",
    "        robot_state = filter_and_concat_robot_state(obs[\"robot_state\"])\n",
    "\n",
    "        img1_batch.append(obs[\"color_image1\"])\n",
    "        img2_batch.append(obs[\"color_image2\"])\n",
    "\n",
    "        actions.append(action)\n",
    "\n",
    "        if len(img1_batch) == batch_size:\n",
    "            img1_features = get_features(np.stack(img1_batch, axis=0))\n",
    "            img2_features = get_features(np.stack(img2_batch, axis=0))\n",
    "\n",
    "            for f1, f2 in zip(img1_features, img2_features):\n",
    "                observation = np.concatenate((robot_state, f1, f2))\n",
    "                observations.append(observation)\n",
    "\n",
    "            img1_batch = []\n",
    "            img2_batch = []\n",
    "\n",
    "        end_index += 1\n",
    "\n",
    "    # Handle any remaining images within each trajectory\n",
    "    if img1_batch:\n",
    "        img1_features = get_features(np.stack(img1_batch, axis=0))\n",
    "        img2_features = get_features(np.stack(img2_batch, axis=0))\n",
    "\n",
    "        for f1, f2 in zip(img1_features, img2_features):\n",
    "            observation = np.concatenate((robot_state, f1, f2))\n",
    "            observations.append(observation)\n",
    "\n",
    "    episode_ends.append(end_index)\n",
    "\n",
    "observations = np.array(observations)\n",
    "actions = np.array(actions)\n",
    "episode_ends = np.array(episode_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.save(\n",
    "    output_file,\n",
    "    observations=observations,\n",
    "    actions=actions,\n",
    "    episode_ends=episode_ends,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
