{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import zarr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from furniture_bench.robot.robot_state import filter_and_concat_robot_state\n",
    "from furniture_bench.perception.image_utils import resize_crop\n",
    "\n",
    "from vip import load_vip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"/home/larsankile/furniture-diffusion/data/\")\n",
    "randomness = \"low\"\n",
    "furniture = \"one_leg\"\n",
    "extension = \".tar.gz\"\n",
    "obs_type = \"image\"\n",
    "env_type = \"real\"\n",
    "filename = furniture + extension\n",
    "\n",
    "\n",
    "input_file = root / \"raw\" / env_type / obs_type / randomness / filename\n",
    "output_file = (\n",
    "    root / \"processed\" / env_type / obs_type / randomness / furniture / \"data.zarr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting one_leg: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "max_samples = 200\n",
    "\n",
    "raw_data, n_samples = [], 0\n",
    "with tarfile.open(input_file, \"r:gz\") as tar:\n",
    "    for member in tqdm(tar, desc=f\"Extracting {furniture}\", total=max_samples):\n",
    "        if (\n",
    "            member.isfile() and \".pkl\" in member.name\n",
    "        ):  # Replace 'your_condition' with actual condition\n",
    "            with tar.extractfile(member) as f:\n",
    "                if f is not None:\n",
    "                    content = f.read()\n",
    "                    data = pickle.loads(content)\n",
    "                    raw_data.append(data)\n",
    "                    n_samples += 1\n",
    "\n",
    "                    if n_samples >= max_samples:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(\"/home/larsankile/furniture-diffusion/data/raw/sim/full/one_leg/low\")\n",
    "output_file = Path(\n",
    "    \"/home/larsankile/furniture-diffusion/data/processed/sim/image/low/one_leg/data.zarr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:02<00:00, 27.64it/s]\n"
     ]
    }
   ],
   "source": [
    "files = list(input_dir.rglob(\"**/*.pkl\"))\n",
    "\n",
    "raw_data = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        raw_data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vip = load_vip(device_id=1).module\n",
    "\n",
    "vip.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(img_batch):\n",
    "    with torch.no_grad():\n",
    "        img_tensor = torch.tensor(\n",
    "            img_batch, dtype=torch.float32, device=\"cuda:1\"\n",
    "        ).permute(0, 3, 1, 2)\n",
    "        features = vip(img_tensor).cpu().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [04:18<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "observations = []\n",
    "actions = []\n",
    "episode_ends = []\n",
    "\n",
    "end_index = 0\n",
    "\n",
    "for data in tqdm(raw_data):\n",
    "    img1_batch = []\n",
    "    img2_batch = []\n",
    "    for obs, action in zip(data[\"observations\"], data[\"actions\"]):\n",
    "        robot_state = filter_and_concat_robot_state(obs[\"robot_state\"])\n",
    "\n",
    "        img1_batch.append(obs[\"color_image1\"])\n",
    "        img2_batch.append(obs[\"color_image2\"])\n",
    "\n",
    "        actions.append(action)\n",
    "\n",
    "        if len(img1_batch) == batch_size:\n",
    "            img1_features = get_features(np.stack(img1_batch, axis=0))\n",
    "            img2_features = get_features(np.stack(img2_batch, axis=0))\n",
    "\n",
    "            for f1, f2 in zip(img1_features, img2_features):\n",
    "                observation = np.concatenate((robot_state, f1, f2))\n",
    "                observations.append(observation)\n",
    "\n",
    "            img1_batch = []\n",
    "            img2_batch = []\n",
    "\n",
    "        end_index += 1\n",
    "\n",
    "    # Handle any remaining images within each trajectory\n",
    "    if img1_batch:\n",
    "        img1_features = get_features(np.stack(img1_batch, axis=0))\n",
    "        img2_features = get_features(np.stack(img2_batch, axis=0))\n",
    "\n",
    "        for f1, f2 in zip(img1_features, img2_features):\n",
    "            observation = np.concatenate((robot_state, f1, f2))\n",
    "            observations.append(observation)\n",
    "\n",
    "    episode_ends.append(end_index)\n",
    "\n",
    "observations = np.array(observations)\n",
    "actions = np.array(actions)\n",
    "episode_ends = np.array(episode_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.save(\n",
    "    output_file,\n",
    "    observations=observations,\n",
    "    actions=actions,\n",
    "    episode_ends=episode_ends,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 530.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35573, 14), (35573, 224, 224, 3), (35573, 224, 224, 3), (35573, 8), (73,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_pos = []\n",
    "image1 = []\n",
    "image2 = []\n",
    "actions = []\n",
    "episode_ends = []\n",
    "\n",
    "end_index = 0\n",
    "\n",
    "for data in tqdm(raw_data):\n",
    "    img1_batch = []\n",
    "    img2_batch = []\n",
    "    for obs, action in zip(data[\"observations\"], data[\"actions\"]):\n",
    "        robot_state = filter_and_concat_robot_state(obs[\"robot_state\"])\n",
    "        agent_pos.append(robot_state)\n",
    "\n",
    "        img1 = obs[\"color_image1\"]\n",
    "        img2 = obs[\"color_image2\"]\n",
    "\n",
    "        if img1.shape != (224, 224, 3):\n",
    "            img1 = resize_crop(img1)\n",
    "            img2 = resize_crop(img2)\n",
    "\n",
    "        image1.append(img1)\n",
    "        image2.append(img2)\n",
    "\n",
    "        actions.append(action)\n",
    "\n",
    "        end_index += 1\n",
    "\n",
    "    episode_ends.append(end_index)\n",
    "\n",
    "agent_pos = np.array(agent_pos)\n",
    "image1 = np.array(image1)\n",
    "image2 = np.array(image2)\n",
    "actions = np.array(actions)\n",
    "episode_ends = np.array(episode_ends)\n",
    "\n",
    "agent_pos.shape, image1.shape, image2.shape, actions.shape, episode_ends.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.save(\n",
    "    output_file,\n",
    "    agent_pos=agent_pos,\n",
    "    image1=image1,\n",
    "    image2=image2,\n",
    "    actions=actions,\n",
    "    episode_ends=episode_ends,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
